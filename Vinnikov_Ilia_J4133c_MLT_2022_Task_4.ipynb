{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\vinni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\vinni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\universal_tagset.zip.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 most important (for example, in terms of TF-IDF metric) words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'alice',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('alice')\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter  1\n",
      "     ('little', 0.17406982794883924)\n",
      "     ('key', 0.15169524014922775)\n",
      "     ('eat', 0.1440408007571977)\n",
      "     ('bats', 0.13330509622302986)\n",
      "     ('like', 0.1276512071624821)\n",
      "     ('think', 0.1276512071624821)\n",
      "     ('way', 0.1276512071624821)\n",
      "     ('door', 0.12067164195789576)\n",
      "     ('see', 0.11604655196589282)\n",
      "     ('bottle', 0.11448396512765768)\n",
      "Chapter  2\n",
      "     ('mouse', 0.31522004680716653)\n",
      "     ('pool', 0.19364979872732435)\n",
      "     ('little', 0.18888067316144894)\n",
      "     ('cats', 0.1694435738864088)\n",
      "     ('swam', 0.15953762598814322)\n",
      "     ('dear', 0.14025659532210416)\n",
      "     ('said', 0.1333275339963169)\n",
      "     ('mabel', 0.1276301007905146)\n",
      "     ('cried', 0.1215702480798422)\n",
      "     ('feet', 0.1215702480798422)\n",
      "Chapter  3\n",
      "     ('mouse', 0.3861039656763269)\n",
      "     ('said', 0.3701672031886347)\n",
      "     ('dodo', 0.3222201511326065)\n",
      "     ('lory', 0.16111007556630325)\n",
      "     ('prizes', 0.15633043822569043)\n",
      "     ('dry', 0.14231764000742336)\n",
      "     ('thimble', 0.12506435058055235)\n",
      "     ('know', 0.11975997750220535)\n",
      "     ('birds', 0.10645091211899584)\n",
      "     ('dinah', 0.10645091211899584)\n",
      "Chapter  4\n",
      "     ('bill', 0.2155073280570946)\n",
      "     ('little', 0.21122643549837986)\n",
      "     ('window', 0.21099149122901248)\n",
      "     ('rabbit', 0.19099562137633636)\n",
      "     ('puppy', 0.18461755482538592)\n",
      "     ('bottle', 0.13590145768141515)\n",
      "     ('fan', 0.13590145768141515)\n",
      "     ('gloves', 0.13590145768141515)\n",
      "     ('chimney', 0.1318696820181328)\n",
      "     ('one', 0.12857261291205732)\n",
      "Chapter  5\n",
      "     ('caterpillar', 0.48032804286681874)\n",
      "     ('said', 0.4409399778059178)\n",
      "     ('pigeon', 0.29222132034137865)\n",
      "     ('serpent', 0.219165990256034)\n",
      "     ('youth', 0.14611066017068933)\n",
      "     ('eggs', 0.12175888347557444)\n",
      "     ('father', 0.10456794349766213)\n",
      "     ('size', 0.09949190459649415)\n",
      "     ('little', 0.09327576453586724)\n",
      "     ('well', 0.08479614957806113)\n",
      "Chapter  6\n",
      "     ('said', 0.3791807020719865)\n",
      "     ('cat', 0.31472141506385143)\n",
      "     ('footman', 0.25485641749241383)\n",
      "     ('baby', 0.21887365179157078)\n",
      "     ('mad', 0.19334347323276882)\n",
      "     ('pig', 0.1853501218126646)\n",
      "     ('duchess', 0.16778430960215746)\n",
      "     ('wow', 0.13901259135949848)\n",
      "     ('like', 0.12908279219471883)\n",
      "     ('cook', 0.12303675569358015)\n",
      "Chapter  7\n",
      "     ('hatter', 0.46591860818443204)\n",
      "     ('dormouse', 0.43154277324225876)\n",
      "     ('said', 0.38234874170060024)\n",
      "     ('hare', 0.2661260025107562)\n",
      "     ('march', 0.2661260025107562)\n",
      "     ('twinkle', 0.1488854167798465)\n",
      "     ('time', 0.1101682815069526)\n",
      "     ('tea', 0.09883121991790983)\n",
      "     ('draw', 0.09589839405383528)\n",
      "     ('clock', 0.09305338548740406)\n",
      "Chapter  8\n",
      "     ('queen', 0.4583387720241304)\n",
      "     ('said', 0.3381389388689538)\n",
      "     ('king', 0.19990812429255192)\n",
      "     ('gardeners', 0.18066384013749873)\n",
      "     ('soldiers', 0.17455073009336694)\n",
      "     ('hedgehog', 0.15808086012031136)\n",
      "     ('cat', 0.1533820934749933)\n",
      "     ('five', 0.1357616789615076)\n",
      "     ('executioner', 0.13549788010312405)\n",
      "     ('three', 0.12849740468859483)\n",
      "Chapter  9\n",
      "     ('turtle', 0.42381513106651386)\n",
      "     ('said', 0.4106757764556235)\n",
      "     ('mock', 0.40811827436034664)\n",
      "     ('gryphon', 0.2817828705454388)\n",
      "     ('duchess', 0.2033540252256824)\n",
      "     ('moral', 0.16552695983522958)\n",
      "     ('queen', 0.16330881712878131)\n",
      "     ('went', 0.09366289638461589)\n",
      "     ('day', 0.07276104203228335)\n",
      "     ('little', 0.07204838183431991)\n",
      "Chapter  10\n",
      "     ('turtle', 0.40008596176900874)\n",
      "     ('mock', 0.38628989412180154)\n",
      "     ('gryphon', 0.3838741861375143)\n",
      "     ('said', 0.2849570240202315)\n",
      "     ('dance', 0.23640939166135774)\n",
      "     ('beautiful', 0.16555281176648637)\n",
      "     ('soup', 0.16555281176648637)\n",
      "     ('join', 0.16366804038094)\n",
      "     ('whiting', 0.14548270256083554)\n",
      "     ('lobster', 0.12729736474073108)\n",
      "Chapter  11\n",
      "     ('king', 0.4082473122858193)\n",
      "     ('hatter', 0.3673646650036406)\n",
      "     ('said', 0.3211808295620677)\n",
      "     ('court', 0.2970525696505318)\n",
      "     ('dormouse', 0.25744556036379423)\n",
      "     ('witness', 0.23059195137885125)\n",
      "     ('queen', 0.11700098011112046)\n",
      "     ('breadandbutter', 0.09901752321684393)\n",
      "     ('tarts', 0.09901752321684393)\n",
      "     ('rabbit', 0.09542332651874039)\n",
      "Chapter  12\n",
      "     ('project', 0.5826938450609705)\n",
      "     ('gutenbergtm', 0.3838924155695806)\n",
      "     ('gutenberg', 0.1919462077847903)\n",
      "     ('works', 0.18839500905205622)\n",
      "     ('electronic', 0.18509098607819066)\n",
      "     ('work', 0.1777797055348493)\n",
      "     ('foundation', 0.15081487754519238)\n",
      "     ('copyright', 0.13024921242539342)\n",
      "     ('terms', 0.12363422469041191)\n",
      "     ('agreement', 0.12339399071879377)\n"
     ]
    }
   ],
   "source": [
    "f = open(\"11-0.txt\", \"r\", encoding=\"utf8\")\n",
    "text = f.read()\n",
    "f.close()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "text = text.lower() # to lower case\n",
    "text = text.replace('\\n', \" \") # replace new line symbols\n",
    "text = re.sub('[^a-z \\’]', \"\", text) # removing non-alphabetic characters\n",
    "text = lemmatizer.lemmatize(text)\n",
    "\n",
    "chapters = text.split(\"chapter\")[13:]\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "X = vectorizer.fit_transform(chapters)\n",
    "\n",
    "\n",
    "tf_idfs = list(dict(zip(vectorizer.get_feature_names(), x.toarray()[0])) for x in X)\n",
    "tf_idfs = list(OrderedDict(sorted(tf_idf.items(), \n",
    "                                  key=lambda kv: kv[1], reverse=True))for tf_idf in tf_idfs)\n",
    "for i in range(len(tf_idfs)):\n",
    "    tf_idf = list(tf_idfs[i].items())[:10]\n",
    "    print(\"Chapter \", i + 1)\n",
    "    for ti in tf_idf:\n",
    "        print( '    ', ti)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 most used verbs in sentences with Alice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most used verbs in sentences with Alice\n",
      "   ('said', 210)\n",
      "   ('went', 43)\n",
      "   ('could', 41)\n",
      "   ('know', 41)\n",
      "   ('would', 40)\n",
      "   ('see', 35)\n",
      "   ('say', 33)\n",
      "   ('began', 30)\n",
      "   ('go', 30)\n",
      "   ('looked', 30)\n"
     ]
    }
   ],
   "source": [
    "f = open(\"11-0.txt\", \"r\", encoding=\"utf8\")\n",
    "text = f.read()\n",
    "f.close()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "text = text.lower() # to lower case\n",
    "text = text.replace('\\n', \" \") # replace new line symbols\n",
    "text = re.sub('[^a-z .\\’]', \"\", text) # removing non-alphabetic characters\n",
    "text = lemmatizer.lemmatize(text)\n",
    "\n",
    "sentences = text.split(\".\")\n",
    "sentences = [s for s in sentences if 'alice' in s]\n",
    "sentences = ' '.join(sentences)\n",
    "vectorizer = CountVectorizer(stop_words=stop_words)\n",
    "X = vectorizer.fit_transform([sentences])\n",
    "counts = dict(zip(vectorizer.get_feature_names(), X.toarray()[0]))\n",
    "counts = OrderedDict(sorted(counts.items(), \n",
    "                                  key=lambda kv: kv[1], reverse=True))\n",
    "\n",
    "counts = [count for count in counts.items() if nltk.pos_tag([count[0]], tagset='universal')[0][1] == \"VERB\"]\n",
    "top10 = counts[:10]\n",
    "print(\"Top 10 most used verbs in sentences with Alice\")\n",
    "for top in top10:\n",
    "    print(\"  \", top)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3f8e440cb4b36429fe7c2047b461cda2a3c3dcadf54d110d570bf5a191ab576e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
